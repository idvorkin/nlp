# This configuration compares LLM output of 2 prompts x 2 GPT models across 3 test cases.
# Learn more: https://promptfoo.dev/docs/configuration/guide
description: "My first eval"

prompts:
  - diff_commit.json

providers:
  - openai:chat:gpt-4.1
  - anthropic:messages:claude-sonnet-4-20250514
  - groq:meta-llama/llama-4-maverick-17b-128e-instruct

tests:
  - assert:
      - type: llm-rubric
        value: ensure the documents are merged well, and there is nothing beyond the merge documents included
