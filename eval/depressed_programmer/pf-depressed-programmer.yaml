# This configuration compares LLM output of 2 prompts x 2 GPT models across 3 test cases.
# Learn more: https://promptfoo.dev/docs/configuration/guide
description: "My first eval"

prompts:
  - "Write 3 spicey jokes about {{topic}}, and explain why they are funny, and explain with why they are spcy. Start with the list of jokes - **then** list the explanations after"

providers:
  - openai:gpt-4o
  - anthropic:messages:claude-3-7-sonnet-20250219
  # - google:gemini-pro
  # - id: openai:chat:meta/llama-3.1-405b-instruct
  - id: groq:meta-llama/llama-4-maverick-17b-128e-instruct
  # This is too slow to be included (12s vs 4s for frontier models and 2s for it running on groq)
  # - id: replicate:meta/meta-llama-3.1-405b-instruct

tests:
  - vars:
      topic: manic depressive programmer
    assert:
      # Foue: ensure that the output is funny
      - type: select-best
        value: ensure that the output is funny
  - vars:
      topic: FAANG programmer in Denmark
    assert:
      # Foue: ensure that the output is funny
      - type: select-best
        value: ensure that the output is funny

  - vars:
      topic: kettle bell weekend warrier and Pavel TaTasoine fan
    assert:
      # For more information on model-graded evals, see https://promptfoo.dev/docs/configuration/expected-outputs/model-graded
      - type: llm-rubric
        value: ensure that the output is funny

  - vars:
      topic: avocado toast
    assert:
      # For more information on assertions, see https://promptfoo.dev/docs/configuration/expected-outputs
      - type: icontains
        value: avocado
      - type: javascript
        value: 1 / (output.length + 1) # prefer shorter outputs
