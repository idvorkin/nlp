{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f14dcd-8e57-48d7-a3be-a0eec0383d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Coding with LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a0db2cdd-dbff-4a62-8824-ceb669184b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ooodles of Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from icecream import ic\n",
    "from rich.console import Console\n",
    "from rich import print\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "import pudb\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "console = Console()\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "from langchain.output_parsers.openai_functions import OutputFunctionsParser\n",
    "from langchain.schema import FunctionMessage\n",
    "\n",
    "\n",
    "from langchain.schema import (\n",
    "    Generation,\n",
    "    OutputParserException,\n",
    ")\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a8207-04ab-4ba5-8627-74a3539ec2e5",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful helpers\n",
    "def model_to_openai_function(cls):\n",
    "    return {\"name\": cls.__name__, \"parameters\": cls.model_json_schema()}\n",
    "\n",
    "\n",
    "class JsonOutputFunctionsParser2(OutputFunctionsParser):\n",
    "    \"\"\"Parse an output as the Json object.\"\"\"\n",
    "\n",
    "    def parse_result(self, result: List[Generation]) -> Any:\n",
    "        function_call_info = super().parse_result(result)\n",
    "        if self.args_only:\n",
    "            try:\n",
    "                # Waiting for this to merge upstream\n",
    "                return json.loads(function_call_info, strict=False)\n",
    "            except (json.JSONDecodeError, TypeError) as exc:\n",
    "                raise OutputParserException(\n",
    "                    f\"Could not parse function call data: {exc}\"\n",
    "                )\n",
    "        function_call_info[\"arguments\"] = json.loads(function_call_info[\"arguments\"])\n",
    "        return function_call_info\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def print_line():\n",
    "    display(HTML(\"<hr>\"))\n",
    "\n",
    "\n",
    "def print_prompt(prompt):\n",
    "    print(\"Prompt:\")\n",
    "    for m in prompt.messages:\n",
    "        print(f\"{type(m)}  {m.prompt}\")\n",
    "def print_function_call(response):\n",
    "    # #     additional_kwargs={'function_call': {'name': 'GetWeather', 'arguments': '{\\n\"City\": \"Spain\"\\n}'}},\n",
    "    function = response.additional_kwargs[\"function_call\"]\n",
    "    print_line()\n",
    "    print (f\"Call: {function['name']}\")\n",
    "    print (function['arguments'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3014a-0e96-4731-a244-58c50db48608",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Langchain - Super cool, we'll use it, but not our focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3ee05-94d2-44b3-a8d9-c794c48a0904",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prompts And Models - CPU and compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e44682-41ec-4eac-a7c8-30c7ba8b6857",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Everyone wants to be a comedian\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessage, \n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "print(joke_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff6eba-d81a-41e4-ab14-f6241f93aecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the program and run on a familiar GPU\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = joke_prompt | model\n",
    "\n",
    "# Run it => Invoke()\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "\n",
    "# Show output\n",
    "print(result)\n",
    "print_line()\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aa45f74c-1c0a-47a8-929b-e2e2482349ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./llama2.bin\n",
      "falcon_model_load: loading model from './llama2.bin' - please wait ...\n",
      "falcon_model_load: n_vocab   = 65024\n",
      "falcon_model_load: n_embd    = 4544\n",
      "falcon_model_load: n_head    = 71\n",
      "falcon_model_load: n_head_kv = 1\n",
      "falcon_model_load: n_layer   = 32\n",
      "falcon_model_load: ftype     = 2\n",
      "falcon_model_load: qntvr     = 0\n",
      "falcon_model_load: ggml ctx size = 3872.64 MB\n",
      "falcon_model_load: memory_size =    32.00 MB, n_mem = 65536\n",
      "falcon_model_load: ........................ done\n",
      "falcon_model_load: model size =  3872.59 MB / num tensors = 196\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Sure, here are two jokes about software engineers:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Why did the software engineer quit her job?\n",
       "\n",
       "She couldn't handle the stress of deadlines and code reviews!\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. How do you make a software engineer feel appreciated?\n",
       "\n",
       "Write them a thank-you note for all the hard work they put into making your life easier.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Sure, here are two jokes about software engineers:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Why did the software engineer quit her job?\n",
       "\n",
       "She couldn't handle the stress of deadlines and code reviews!\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. How do you make a software engineer feel appreciated?\n",
       "\n",
       "Write them a thank-you note for all the hard work they put into making your life easier.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile the program and run on our CPU\n",
    "\n",
    "local_model = GPT4All(model=\"./llama2.bin\")\n",
    "local_chain = joke_prompt | local_model\n",
    "\n",
    "# Run it => Invoke()\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = local_chain.invoke({\"topic\": topic, \"count\": count})\n",
    "\n",
    "# Show output\n",
    "print_line()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19dcc4-640e-4ffd-8167-263cafd48103",
   "metadata": {},
   "source": [
    "### Functions - The I/O System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c1a76-0863-4aaf-8ca9-f74fa39cd39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The rain in spain\n",
    "# Tell the model the \"OS\" supports getting the weather\n",
    "\n",
    "\n",
    "# define a callable function\n",
    "class GetWeather(BaseModel):\n",
    "    City: str\n",
    "\n",
    "\n",
    "get_weather = model_to_openai_function(GetWeather)\n",
    "\n",
    "weather_prompt_template = \"What's the weather in {place}\"\n",
    "model = ChatOpenAI()\n",
    "weather_prompt = ChatPromptTemplate.from_template(weather_prompt_template)\n",
    "\n",
    "chain = weather_prompt | model.bind(\n",
    "    functions=[get_weather]  # tell model we can call it.\n",
    ")\n",
    "\n",
    "\n",
    "print (\"Show the full response:\")\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(response)\n",
    "\n",
    "print (\"Include an output parser in the chain:\")\n",
    "print_function_call(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416bd24-8278-49df-ba06-84b2e592e2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Woah  - Did you see  the bug?\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Do \"some more programming\"\n",
    "weather_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"When an API takes a city, infer an appropritiate city\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(weather_prompt_template),\n",
    "    ]\n",
    ")\n",
    "chain = weather_prompt | model.bind(functions=[get_weather])\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(\"Output\")\n",
    "print(response)\n",
    "print_line()\n",
    "print (response.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489308bf-a7dc-43fb-adf8-473db4e9adf6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Back to our functions\n",
    "\n",
    "weather_with_data = weather_prompt.copy()\n",
    "\n",
    "# Update prompt with AI's desire to call a function\n",
    "weather_with_data.append(response)\n",
    "\n",
    "# Need to make tomorrow's cut, just stamp this please :)\n",
    "# Will come back and make a dispatcher and call actual functions\n",
    "\n",
    "weather_with_data.append(\n",
    "    FunctionMessage(name=\"GetWeather\", content=\"5 degrees and rainy\")\n",
    ")\n",
    "\n",
    "print(weather_with_data)\n",
    "\n",
    "\n",
    "chain = weather_with_data | model.bind(\n",
    "    functions=[get_weather]\n",
    ")  # tell model we can call it.\n",
    "\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print_line()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430962af-0803-4277-af45-aa1688cfcdfa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - Why do we seperate view from model?\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str\n",
    "    punch_line: str\n",
    "    reason_joke_is_funny: str\n",
    "\n",
    "\n",
    "class GetJokes(BaseModel):\n",
    "    count: int\n",
    "    jokes: List[Joke]\n",
    "\n",
    "\n",
    "get_jokes = model_to_openai_function(GetJokes)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "chain = prompt | model.bind(functions=[get_jokes]) | JsonOutputFunctionsParser2()\n",
    "print(prompt.messages)\n",
    "response = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "print(\"Output\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a7f7e-eee7-4abe-a325-386fb8d7bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innovate - What's better then doing math with a calculator?\n",
    "\n",
    "\n",
    "solve_math_with_python = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Write code to solve the users problem. the last line of the python  program should print the answer. Do not use sympy\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"What is the 217th prime\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ExecutePythonCode(BaseModel):\n",
    "    valid_python: str\n",
    "    code_explanation: str\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\").bind(\n",
    "    function_call={\"name\": \"ExecutePythonCode\"},  # tell gpt to use this model\n",
    "    functions=[model_to_openai_function(ExecutePythonCode)],\n",
    ")\n",
    "\n",
    "\n",
    "# JsonOutputFunctionParser2 == PrettyPrintOutput\n",
    "\n",
    "chain = solve_math_with_python | model | JsonOutputFunctionsParser2()\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response[\"code_explanation\"])\n",
    "print_line()\n",
    "\n",
    "valid_python = response[\"valid_python\"]\n",
    "print(valid_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6cb89-edf8-4285-93e1-321a94b7888c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Woah Nelly  ... are you sure you want to do this??\n",
    "print_line()\n",
    "# input(\"Are you sure you want to run this code??\")\n",
    "exec(valid_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873feb1-d320-4e7e-992d-ff32737a33a5",
   "metadata": {},
   "source": [
    "### Memory - Staying in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093eac8-3709-4410-9f24-5a2b54f5a216",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory - # no memory\n",
    "model = ChatOpenAI().bind(temperature=0)\n",
    "prompt_template = \"Tell me another joke\"\n",
    "last_prompt = \"\"\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    print(f\"Prompt {i}\\n\", prompt.messages)\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({})\n",
    "\n",
    "    print(f\"Response {i}\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa0d47-c408-4514-8463-b9fac7e6bfe4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# The memory module!\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "# memory.save_context({\"input\":\"I like software engineer jokes\"} , {\"output\":\"That's odd, but fine\"})\n",
    "memory.add_user_message(\"I like software engineering jokes\")\n",
    "memory.add_ai_message(\"OK!\")\n",
    "print(memory.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912f985-8b0c-4bec-b56b-557a672b3b0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory w/Memory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "memory.add_user_message(\"I like software engineering jokes\")\n",
    "memory.add_ai_message(\"OK!\")\n",
    "\n",
    "# human always says the same thing\n",
    "human_says = \"tell me another joke\"\n",
    "prompt = \"\"  # have it outside loop so we can print it\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "\n",
    "    # build prompt, we'll print the last one ...\n",
    "    prompt = ChatPromptTemplate.from_messages(memory.messages)\n",
    "    prompt.append(human_says)\n",
    "    # run the chain\n",
    "    chain = prompt | model\n",
    "\n",
    "    response = chain.invoke({})\n",
    "    print(f\"Response {i}\\n\", response.content)\n",
    "\n",
    "    # store history\n",
    "    memory.add_user_message(human_says)\n",
    "    memory.add_ai_message(response.content)\n",
    "\n",
    "print_line()\n",
    "print_line()\n",
    "print(f\"Final Prompt\\n\", prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11df5ec-0ebb-47c8-b374-0befb237b82b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - A new kind of lossy compression\n",
    "\n",
    "semantic_compression_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a compressing chat model, summarize the entire conversation into a paragaph\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for m in memory.messages:\n",
    "    semantic_compression_prompt.append(m)\n",
    "\n",
    "chain = semantic_compression_prompt | model\n",
    "\n",
    "response = chain.invoke({})\n",
    "print(\"Semantic Compression of Conversation \\n\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca5fe0-7eb8-48b9-8ab9-d9bff6020e7f",
   "metadata": {},
   "source": [
    "### Retrievel and Index - Exceeding RAM and Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc6d0a-c229-45a6-a69e-e41ba9a775ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Raw Data -> Index -> Retreive:  A picture is worth a thousand words\n",
    "Image(filename=\"./images/retrieval_pipeline.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "53456a5b-a781-4433-a165-57739cbe5bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'puppy'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kitten'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'puppy'\u001b[0m, \u001b[32m'kitten'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find some young pets - using english\n",
    "\n",
    "# Data Set small enough we can use raw data run the query and retrieval in one fell swoop\n",
    "things_we_saw = [\n",
    "    \"dog\", \"cat\",\"zebra\", \"puppy\",\"calf\", \"puppies\" , \"kitten\",  \"cow\", \"desk\",\"rubber band\",\"mouse\", \"airpods\", \"smelly socks\", \"sandals\",\"jacket\", \"cats\", \"young dog\", \"young cat\", \"young hamster\"\n",
    "]\n",
    "\n",
    "# The query to retreive  - but not super\n",
    "retrieval_query=\"puppy|kitten\"\n",
    "young_pets = [ item for item in things_we_saw if re.match(retrieval_query, item) ]\n",
    "\n",
    "\n",
    "print (young_pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b2e63d61-4706-455c-b28a-8e21d1f55994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find some young pets - using english\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "\n",
    "# Reperesent words using embedding (if time see pictures)\n",
    "VectorStore =  FAISS\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = VectorStore.from_texts(things_we_saw, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7c490-07eb-45df-a7e9-3780e5b12cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.similarity_search_with_relevance_scores(\"young dogs\", k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1866b-9eaf-4ad5-a8ed-030b3fc11e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve using LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a5f20-462b-4c71-90ec-553b536a4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great Walk through of Word Embeddings\n",
    "\n",
    "https://investigate.ai/text-analysis/word-embeddings/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e39b3c-2971-4083-af42-df5874921a43",
   "metadata": {},
   "source": [
    "### Planning - Workflow and Verifcation and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136af2f-a9e3-4b11-8e50-9ccfd768bc1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Help me write a talk\n",
    "\n",
    "our_task = \"Write a talk on coding with large language models in markdown\"\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(our_task)\n",
    "result = (prompt | model) .invoke({})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aeaac57c-dd9f-4b2b-bfdc-580a3c3a4495",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Start by introducing the topic of coding with large language models and its significance in the field of \n",
       "artificial intelligence.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Explain what large language models are and how they have revolutionized natural language processing tasks.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Discuss the benefits of using large language models in coding, such as automated code generation, code \n",
       "completion, and bug detection.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Explain the different approaches to coding with large language models, including fine-tuning, prompt \n",
       "engineering, and zero-shot learning.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Provide examples of popular large language models like GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, and T5, and discuss their applications in \n",
       "coding.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. Discuss the challenges and limitations of using large language models in coding, such as bias, ethics, and the \n",
       "need for careful evaluation.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. Present case studies or real-world examples where coding with large language models has been successful.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>. Discuss potential future developments and advancements in coding with large language models, such as model \n",
       "optimization and hybrid approaches.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>. Provide practical tips and best practices for developers who want to start coding with large language models, \n",
       "including data preprocessing, model selection, and fine-tuning techniques.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>. Discuss the potential impact of coding with large language models on the software development industry and the \n",
       "job market.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>. Conclude the talk by summarizing the main points discussed and emphasizing the importance of staying updated \n",
       "with the latest advancements in coding with large language models.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>. Open the floor for questions and engage in a discussion with the audience to address any concerns or queries \n",
       "they may have.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m. Start by introducing the topic of coding with large language models and its significance in the field of \n",
       "artificial intelligence.\n",
       "\u001b[1;36m2\u001b[0m. Explain what large language models are and how they have revolutionized natural language processing tasks.\n",
       "\u001b[1;36m3\u001b[0m. Discuss the benefits of using large language models in coding, such as automated code generation, code \n",
       "completion, and bug detection.\n",
       "\u001b[1;36m4\u001b[0m. Explain the different approaches to coding with large language models, including fine-tuning, prompt \n",
       "engineering, and zero-shot learning.\n",
       "\u001b[1;36m5\u001b[0m. Provide examples of popular large language models like GPT-\u001b[1;36m3\u001b[0m, GPT-\u001b[1;36m2\u001b[0m, and T5, and discuss their applications in \n",
       "coding.\n",
       "\u001b[1;36m6\u001b[0m. Discuss the challenges and limitations of using large language models in coding, such as bias, ethics, and the \n",
       "need for careful evaluation.\n",
       "\u001b[1;36m7\u001b[0m. Present case studies or real-world examples where coding with large language models has been successful.\n",
       "\u001b[1;36m8\u001b[0m. Discuss potential future developments and advancements in coding with large language models, such as model \n",
       "optimization and hybrid approaches.\n",
       "\u001b[1;36m9\u001b[0m. Provide practical tips and best practices for developers who want to start coding with large language models, \n",
       "including data preprocessing, model selection, and fine-tuning techniques.\n",
       "\u001b[1;36m10\u001b[0m. Discuss the potential impact of coding with large language models on the software development industry and the \n",
       "job market.\n",
       "\u001b[1;36m11\u001b[0m. Conclude the talk by summarizing the main points discussed and emphasizing the importance of staying updated \n",
       "with the latest advancements in coding with large language models.\n",
       "\u001b[1;36m12\u001b[0m. Open the floor for questions and engage in a discussion with the audience to address any concerns or queries \n",
       "they may have.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chain of thought, \n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate(\n",
    "                                messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Given a task, make a list of steps to accomplish it\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(our_task)])\n",
    "result = (prompt | model) .invoke({})\n",
    "the_plan = result.content\n",
    "print(the_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "062c6597-662a-4c39-bf1e-9301d432ff95",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your plan for the talk on coding with large language models in markdown is comprehensive and covers all the \n",
       "necessary points. However, I would like to suggest a few changes to enhance the clarity and effectiveness of the \n",
       "talk:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Start by introducing the topic of coding with large language models and its significance in the field of \n",
       "artificial intelligence and software development.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Explain what large language models are and how they have revolutionized natural language processing tasks, \n",
       "particularly in the context of coding.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Discuss the benefits of using large language models in coding, such as automated code generation, code \n",
       "completion, and bug detection, and provide concrete examples to illustrate these benefits.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Explain the different approaches to coding with large language models, including fine-tuning, prompt \n",
       "engineering, and zero-shot learning, and highlight their respective strengths and limitations.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Provide examples of popular large language models like GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, and T5, and discuss their specific \n",
       "applications in coding, such as generating code snippets, assisting with documentation, and facilitating code \n",
       "reviews.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. Discuss the challenges and limitations of using large language models in coding, such as bias, ethics, and the \n",
       "need for careful evaluation, and emphasize the importance of responsible and ethical use of these models.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. Present case studies or real-world examples where coding with large language models has been successful, \n",
       "showcasing the practical benefits and impact on software development.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>. Discuss potential future developments and advancements in coding with large language models, such as model \n",
       "optimization techniques, hybrid approaches combining traditional programming with language models, and \n",
       "collaborative coding with multiple models.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>. Provide practical tips and best practices for developers who want to start coding with large language models, \n",
       "including data preprocessing, model selection, and fine-tuning techniques, and recommend available resources and \n",
       "tools for further exploration.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>. Discuss the potential impact of coding with large language models on the software development industry and the \n",
       "job market, addressing both the opportunities and potential challenges that may arise.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>. Conclude the talk by summarizing the main points discussed and emphasizing the importance of continuous \n",
       "learning and adaptation to the evolving landscape of coding with large language models.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>. Open the floor for questions and engage in a discussion with the audience to address any concerns or queries \n",
       "they may have, encouraging them to share their experiences and perspectives on the topic.\n",
       "\n",
       "By incorporating these changes, your talk will be more engaging, informative, and impactful for the audience. Good \n",
       "luck with your presentation!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your plan for the talk on coding with large language models in markdown is comprehensive and covers all the \n",
       "necessary points. However, I would like to suggest a few changes to enhance the clarity and effectiveness of the \n",
       "talk:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Start by introducing the topic of coding with large language models and its significance in the field of \n",
       "artificial intelligence and software development.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. Explain what large language models are and how they have revolutionized natural language processing tasks, \n",
       "particularly in the context of coding.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. Discuss the benefits of using large language models in coding, such as automated code generation, code \n",
       "completion, and bug detection, and provide concrete examples to illustrate these benefits.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. Explain the different approaches to coding with large language models, including fine-tuning, prompt \n",
       "engineering, and zero-shot learning, and highlight their respective strengths and limitations.\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. Provide examples of popular large language models like GPT-\u001b[1;36m3\u001b[0m, GPT-\u001b[1;36m2\u001b[0m, and T5, and discuss their specific \n",
       "applications in coding, such as generating code snippets, assisting with documentation, and facilitating code \n",
       "reviews.\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. Discuss the challenges and limitations of using large language models in coding, such as bias, ethics, and the \n",
       "need for careful evaluation, and emphasize the importance of responsible and ethical use of these models.\n",
       "\n",
       "\u001b[1;36m7\u001b[0m. Present case studies or real-world examples where coding with large language models has been successful, \n",
       "showcasing the practical benefits and impact on software development.\n",
       "\n",
       "\u001b[1;36m8\u001b[0m. Discuss potential future developments and advancements in coding with large language models, such as model \n",
       "optimization techniques, hybrid approaches combining traditional programming with language models, and \n",
       "collaborative coding with multiple models.\n",
       "\n",
       "\u001b[1;36m9\u001b[0m. Provide practical tips and best practices for developers who want to start coding with large language models, \n",
       "including data preprocessing, model selection, and fine-tuning techniques, and recommend available resources and \n",
       "tools for further exploration.\n",
       "\n",
       "\u001b[1;36m10\u001b[0m. Discuss the potential impact of coding with large language models on the software development industry and the \n",
       "job market, addressing both the opportunities and potential challenges that may arise.\n",
       "\n",
       "\u001b[1;36m11\u001b[0m. Conclude the talk by summarizing the main points discussed and emphasizing the importance of continuous \n",
       "learning and adaptation to the evolving landscape of coding with large language models.\n",
       "\n",
       "\u001b[1;36m12\u001b[0m. Open the floor for questions and engage in a discussion with the audience to address any concerns or queries \n",
       "they may have, encouraging them to share their experiences and perspectives on the topic.\n",
       "\n",
       "By incorporating these changes, your talk will be more engaging, informative, and impactful for the audience. Good \n",
       "luck with your presentation!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "                                    messages=[\n",
    "        SystemMessage( content=\n",
    "            \"Given a task an d a plan make recommendation changes with reasoning. First message will be the task, the next the plan\"\n",
    "        ),\n",
    "        HumanMessage(content=our_task),\n",
    "        HumanMessage(content=the_plan)])\n",
    "result = (prompt | model) .invoke({})\n",
    "the_plan = result.content\n",
    "print(the_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd578e6-034c-445e-b729-03f86b5d03c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
