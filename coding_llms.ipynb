{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f14dcd-8e57-48d7-a3be-a0eec0383d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helps - Coding with LLMs 301\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0db2cdd-dbff-4a62-8824-ceb669184b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ooodles of Imports\n",
    "import os\n",
    "import json\n",
    "from icecream import ic\n",
    "from rich.console import Console\n",
    "from rich import print\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "import pudb\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "console = Console()\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "from langchain.output_parsers.openai_functions import OutputFunctionsParser\n",
    "from langchain.schema import FunctionMessage\n",
    "\n",
    "\n",
    "from langchain.schema import (\n",
    "    Generation,\n",
    "    OutputParserException,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5a8207-04ab-4ba5-8627-74a3539ec2e5",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful helpers\n",
    "def model_to_openai_function(cls):\n",
    "    return {\"name\": cls.__name__, \"parameters\": cls.model_json_schema()}\n",
    "\n",
    "\n",
    "class JsonOutputFunctionsParser2(OutputFunctionsParser):\n",
    "    \"\"\"Parse an output as the Json object.\"\"\"\n",
    "\n",
    "    def parse_result(self, result: List[Generation]) -> Any:\n",
    "        function_call_info = super().parse_result(result)\n",
    "        if self.args_only:\n",
    "            try:\n",
    "                # Waiting for this to merge upstream\n",
    "                return json.loads(function_call_info, strict=False)\n",
    "            except (json.JSONDecodeError, TypeError) as exc:\n",
    "                raise OutputParserException(\n",
    "                    f\"Could not parse function call data: {exc}\"\n",
    "                )\n",
    "        function_call_info[\"arguments\"] = json.loads(function_call_info[\"arguments\"])\n",
    "        return function_call_info\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def print_line():\n",
    "    display(HTML(\"<hr>\"))\n",
    "\n",
    "\n",
    "def print_prompt(prompt):\n",
    "    print(\"Prompt:\")\n",
    "    for m in prompt.messages:\n",
    "        print(f\"{type(m)}  {m.prompt}\")\n",
    "def print_function_call(response):\n",
    "    # #     additional_kwargs={'function_call': {'name': 'GetWeather', 'arguments': '{\\n\"City\": \"Spain\"\\n}'}},\n",
    "    function = response.additional_kwargs[\"function_call\"]\n",
    "    print_line()\n",
    "    print (f\"Call: {function['name']}\")\n",
    "    print (function['arguments'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3014a-0e96-4731-a244-58c50db48608",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Langchain - Super cool, we'll use it, but not our focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3ee05-94d2-44b3-a8d9-c794c48a0904",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prompts - The compiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e44682-41ec-4eac-a7c8-30c7ba8b6857",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'count'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">output_parser</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tell me {count} jokes about {topic}'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">template_format</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'f-string'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">validate_template</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'count'\u001b[0m, \u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33moutput_parser\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mtemplate\u001b[0m=\u001b[32m'tell me \u001b[0m\u001b[32m{\u001b[0m\u001b[32mcount\u001b[0m\u001b[32m}\u001b[0m\u001b[32m jokes about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[33mtemplate_format\u001b[0m=\u001b[32m'f-string'\u001b[0m,\n",
       "            \u001b[33mvalidate_template\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Everyone wants to be a comedian\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessage, \n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "print(joke_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ff6eba-d81a-41e4-ab14-f6241f93aecc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compile the program and run on a familiar GPU\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m chain \u001b[38;5;241m=\u001b[39m joke_prompt \u001b[38;5;241m|\u001b[39m model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run it => Invoke()\u001b[39;00m\n",
      "File \u001b[0;32m~/homebrew/lib/python3.10/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/homebrew/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "# Compile the program and run on a familiar GPU\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = joke_prompt | model\n",
    "\n",
    "# Run it => Invoke()\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "\n",
    "# Show output\n",
    "print(result)\n",
    "print_line()\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45f74c-1c0a-47a8-929b-e2e2482349ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compile the program and run on our CPU\n",
    "\n",
    "local_model = GPT4All(model=\"./llama2.bin\")\n",
    "local_chain = joke_prompt | local_model\n",
    "\n",
    "# Run it => Invoke()\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = local_chain.invoke({\"topic\": topic, \"count\": count})\n",
    "\n",
    "# Show output\n",
    "print_line()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19dcc4-640e-4ffd-8167-263cafd48103",
   "metadata": {},
   "source": [
    "### Functions - The operating system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c1a76-0863-4aaf-8ca9-f74fa39cd39a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The rain in spain\n",
    "# Tell the model the \"OS\" supports getting the weather\n",
    "\n",
    "\n",
    "# define a callable function\n",
    "class GetWeather(BaseModel):\n",
    "    City: str\n",
    "\n",
    "\n",
    "get_weather = model_to_openai_function(GetWeather)\n",
    "\n",
    "weather_prompt_template = \"What's the weather in {place}\"\n",
    "model = ChatOpenAI()\n",
    "weather_prompt = ChatPromptTemplate.from_template(weather_prompt_template)\n",
    "\n",
    "chain = weather_prompt | model.bind(\n",
    "    functions=[get_weather]  # tell model we can call it.\n",
    ")\n",
    "\n",
    "\n",
    "print (\"Show the full response:\")\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(response)\n",
    "\n",
    "print (\"Include an output parser in the chain:\")\n",
    "print_function_call(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416bd24-8278-49df-ba06-84b2e592e2fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Woah  - Did you see  the bug?\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Do \"some more programming\"\n",
    "weather_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"When an API takes a city, infer an appropritiate city\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(weather_prompt_template),\n",
    "    ]\n",
    ")\n",
    "chain = weather_prompt | model.bind(functions=[get_weather])\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(\"Output\")\n",
    "print(response)\n",
    "print_line()\n",
    "print (response.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489308bf-a7dc-43fb-adf8-473db4e9adf6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Back to our functions\n",
    "\n",
    "weather_with_data = weather_prompt.copy()\n",
    "\n",
    "# Update prompt with AI's desire to call a function\n",
    "weather_with_data.append(response)\n",
    "\n",
    "# Need to make tomorrow's cut, just stamp this please :)\n",
    "# Will come back and make a dispatcher and call actual functions\n",
    "\n",
    "weather_with_data.append(\n",
    "    FunctionMessage(name=\"GetWeather\", content=\"5 degrees and rainy\")\n",
    ")\n",
    "\n",
    "print(weather_with_data)\n",
    "\n",
    "\n",
    "chain = weather_with_data | model.bind(\n",
    "    functions=[get_weather]\n",
    ")  # tell model we can call it.\n",
    "\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print_line()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430962af-0803-4277-af45-aa1688cfcdfa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - Why do we seperate view from model?\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str\n",
    "    punch_line: str\n",
    "    reason_joke_is_funny: str\n",
    "\n",
    "\n",
    "class GetJokes(BaseModel):\n",
    "    count: int\n",
    "    jokes: List[Joke]\n",
    "\n",
    "\n",
    "get_jokes = model_to_openai_function(GetJokes)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "chain = prompt | model.bind(functions=[get_jokes]) | JsonOutputFunctionsParser2()\n",
    "print(prompt.messages)\n",
    "response = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "print(\"Output\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a7f7e-eee7-4abe-a325-386fb8d7bcf8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - What's better then doing math with a calculator?\n",
    "\n",
    "\n",
    "solve_math_with_python = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Write code to solve the users problem. the last line of the python  program should print the answer. Do not use sympy\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"What is the 217th prime\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ExecutePythonCode(BaseModel):\n",
    "    valid_python: str\n",
    "    code_explanation: str\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\").bind(\n",
    "    function_call={\"name\": \"ExecutePythonCode\"},  # tell gpt to use this model\n",
    "    functions=[model_to_openai_function(ExecutePythonCode)],\n",
    ")\n",
    "\n",
    "\n",
    "# JsonOutputFunctionParser2 == PrettyPrintOutput\n",
    "\n",
    "chain = solve_math_with_python | model | JsonOutputFunctionsParser2()\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response[\"code_explanation\"])\n",
    "print_line()\n",
    "\n",
    "valid_python = response[\"valid_python\"]\n",
    "print(valid_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6cb89-edf8-4285-93e1-321a94b7888c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ugh ... are you sure you want to do this??\n",
    "print_line()\n",
    "# input(\"Are you sure you want to run this code??\")\n",
    "exec(valid_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873feb1-d320-4e7e-992d-ff32737a33a5",
   "metadata": {},
   "source": [
    "### Memory - Do you remember "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093eac8-3709-4410-9f24-5a2b54f5a216",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory - # no memory\n",
    "model = ChatOpenAI().bind(temperature=0)\n",
    "prompt_template = \"Tell me another joke\"\n",
    "last_prompt = \"\"\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    print(f\"Prompt {i}\\n\", prompt.messages)\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({})\n",
    "\n",
    "    print(f\"Response {i}\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa0d47-c408-4514-8463-b9fac7e6bfe4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# The memory module!\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "# memory.save_context({\"input\":\"I like software engineer jokes\"} , {\"output\":\"That's odd, but fine\"})\n",
    "memory.add_user_message(\"I like software engineering jokes\")\n",
    "memory.add_ai_message(\"OK!\")\n",
    "print(memory.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912f985-8b0c-4bec-b56b-557a672b3b0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory w/Memory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "memory.add_user_message(\"I like software engineering jokes\")\n",
    "memory.add_ai_message(\"OK!\")\n",
    "\n",
    "# human always says the same thing\n",
    "human_says = \"tell me another joke\"\n",
    "prompt = \"\"  # have it outside loop so we can print it\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "\n",
    "    # build prompt, we'll print the last one ...\n",
    "    prompt = ChatPromptTemplate.from_messages(memory.messages)\n",
    "    prompt.append(human_says)\n",
    "    # run the chain\n",
    "    chain = prompt | model\n",
    "\n",
    "    response = chain.invoke({})\n",
    "    print(f\"Response {i}\\n\", response.content)\n",
    "\n",
    "    # store history\n",
    "    memory.add_user_message(human_says)\n",
    "    memory.add_ai_message(response.content)\n",
    "\n",
    "print_line()\n",
    "print_line()\n",
    "print(f\"Final Prompt\\n\", prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11df5ec-0ebb-47c8-b374-0befb237b82b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Semantic Compression - But we don't have infinite memory...\n",
    "\n",
    "semantic_compression_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a compressing chat model, summarize the entire conversation into a paragaph\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for m in memory.messages:\n",
    "    semantic_compression_prompt.append(m)\n",
    "\n",
    "chain = semantic_compression_prompt | model\n",
    "\n",
    "response = chain.invoke({})\n",
    "print(\"Semantic Compression of Conversation \\n\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca5fe0-7eb8-48b9-8ab9-d9bff6020e7f",
   "metadata": {},
   "source": [
    "### Retrievel - Where was thats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
