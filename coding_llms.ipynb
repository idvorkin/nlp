{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f14dcd-8e57-48d7-a3be-a0eec0383d93",
   "metadata": {},
   "source": [
    "### Helps - Coding with LLMs 301\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0db2cdd-dbff-4a62-8824-ceb669184b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ooodles of Imports\n",
    "import os\n",
    "import json\n",
    "from icecream import ic\n",
    "import typer\n",
    "from rich.console import Console\n",
    "from rich import print\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "import pudb\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "console = Console()\n",
    "app = typer.Typer()\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "from langchain.output_parsers.openai_functions import OutputFunctionsParser\n",
    "from langchain.schema import FunctionMessage\n",
    "\n",
    "\n",
    "from langchain.schema import (\n",
    "    Generation,\n",
    "    OutputParserException,\n",
    ")\n",
    "import mercury as mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5a8207-04ab-4ba5-8627-74a3539ec2e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Useful helpers\n",
    "def model_to_openai_function(cls):\n",
    "    return {\"name\": cls.__name__, \"parameters\": cls.model_json_schema()}\n",
    "\n",
    "\n",
    "class JsonOutputFunctionsParser2(OutputFunctionsParser):\n",
    "    \"\"\"Parse an output as the Json object.\"\"\"\n",
    "\n",
    "    def parse_result(self, result: List[Generation]) -> Any:\n",
    "        function_call_info = super().parse_result(result)\n",
    "        if self.args_only:\n",
    "            try:\n",
    "                # Waiting for this to merge upstream\n",
    "                return json.loads(function_call_info, strict=False)\n",
    "            except (json.JSONDecodeError, TypeError) as exc:\n",
    "                raise OutputParserException(\n",
    "                    f\"Could not parse function call data: {exc}\"\n",
    "                )\n",
    "        function_call_info[\"arguments\"] = json.loads(function_call_info[\"arguments\"])\n",
    "        return function_call_info\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def print_line():\n",
    "    display(HTML(\"<hr>\"))\n",
    "def print_prompt(prompt):\n",
    "    print (\"Prompt:\")\n",
    "    for m in prompt.messages:\n",
    "        print(f\"{type(m)}  {m.prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3014a-0e96-4731-a244-58c50db48608",
   "metadata": {},
   "source": [
    "### Langchain - Super cool, we'll use it, but not our focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3ee05-94d2-44b3-a8d9-c794c48a0904",
   "metadata": {},
   "source": [
    "### Compilers - Prompts and Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e44682-41ec-4eac-a7c8-30c7ba8b6857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'count'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">output_parser</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tell me {count} jokes about {topic}'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">template_format</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'f-string'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">validate_template</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'count'\u001b[0m, \u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33moutput_parser\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mtemplate\u001b[0m=\u001b[32m'tell me \u001b[0m\u001b[32m{\u001b[0m\u001b[32mcount\u001b[0m\u001b[32m}\u001b[0m\u001b[32m jokes about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[33mtemplate_format\u001b[0m=\u001b[32m'f-string'\u001b[0m,\n",
       "            \u001b[33mvalidate_template\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Everyone wants to be a comedian\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "print(joke_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ff6eba-d81a-41e4-ab14-f6241f93aecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Sure, here are two jokes about software engineers:\\n\\n1. Why do software engineers prefer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">gardening?\\nBecause they love to \"debug\" the plants!\\n\\n2. Why did the software engineer go broke?\\nBecause he had </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">way too many \"bugs\" in his code and couldn\\'t afford the exterminator!'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Sure, here are two jokes about software engineers:\\n\\n1. Why do software engineers prefer \u001b[0m\n",
       "\u001b[32mgardening?\\nBecause they love to \"debug\" the plants!\\n\\n2. Why did the software engineer go broke?\\nBecause he had \u001b[0m\n",
       "\u001b[32mway too many \"bugs\" in his code and couldn\\'t afford the exterminator!'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sure, here are two jokes about software engineers:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Why do software engineers prefer gardening?\n",
       "Because they love to <span style=\"color: #008000; text-decoration-color: #008000\">\"debug\"</span> the plants!\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Why did the software engineer go broke?\n",
       "Because he had way too many <span style=\"color: #008000; text-decoration-color: #008000\">\"bugs\"</span> in his code and couldn't afford the exterminator!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sure, here are two jokes about software engineers:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Why do software engineers prefer gardening?\n",
       "Because they love to \u001b[32m\"debug\"\u001b[0m the plants!\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. Why did the software engineer go broke?\n",
       "Because he had way too many \u001b[32m\"bugs\"\u001b[0m in his code and couldn't afford the exterminator!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile the program and run on a familiar GPU\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = joke_prompt | model\n",
    "\n",
    "# Run it => Invoke()\n",
    "\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "print(result)\n",
    "print_line()\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa45f74c-1c0a-47a8-929b-e2e2482349ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./llama2.bin\n",
      "falcon_model_load: loading model from './llama2.bin' - please wait ...\n",
      "falcon_model_load: n_vocab   = 65024\n",
      "falcon_model_load: n_embd    = 4544\n",
      "falcon_model_load: n_head    = 71\n",
      "falcon_model_load: n_head_kv = 1\n",
      "falcon_model_load: n_layer   = 32\n",
      "falcon_model_load: ftype     = 2\n",
      "falcon_model_load: qntvr     = 0\n",
      "falcon_model_load: ggml ctx size = 3872.64 MB\n",
      "falcon_model_load: memory_size =    32.00 MB, n_mem = 65536\n",
      "falcon_model_load: ."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[82384]: Class GGMLMetalClass is implemented in both /Users/idvorkin/homebrew/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x13f54c208) and /Users/idvorkin/homebrew/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x13f978208). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................... done\n",
      "falcon_model_load: model size =  3872.59 MB / num tensors = 196\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Sure, here are two jokes for you:\\n\\n1. Why do software engineers prefer dark mode?\\nBecause light </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attracts bugs!\\n\\n2. Why did the software engineer go broke?\\nBecause he lost his domain in a bet and couldn't </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">afford to buy a new one!\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m\"Sure\u001b[0m\u001b[32m, here are two jokes for you:\\n\\n1. Why do software engineers prefer dark mode?\\nBecause light \u001b[0m\n",
       "\u001b[32mattracts bugs!\\n\\n2. Why did the software engineer go broke?\\nBecause he lost his domain in a bet and couldn't \u001b[0m\n",
       "\u001b[32mafford to buy a new one!\"\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Sure, here are two jokes for you:\\n\\n1. Why do software engineers prefer dark mode?\\nBecause light </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attracts bugs!\\n\\n2. Why did the software engineer go broke?\\nBecause he lost his domain in a bet and couldn't </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">afford to buy a new one!\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">example</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m\"Sure\u001b[0m\u001b[32m, here are two jokes for you:\\n\\n1. Why do software engineers prefer dark mode?\\nBecause light \u001b[0m\n",
       "\u001b[32mattracts bugs!\\n\\n2. Why did the software engineer go broke?\\nBecause he lost his domain in a bet and couldn't \u001b[0m\n",
       "\u001b[32mafford to buy a new one!\"\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexample\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile the program and run on our CPU\n",
    "local_model = GPT4All(model=\"./llama2.bin\")\n",
    "local_chain = joke_prompt | model\n",
    "\n",
    "\n",
    "result = local_chain.invoke({\"topic\": topic, \"count\": count})\n",
    "print(result)\n",
    "print_line()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19dcc4-640e-4ffd-8167-263cafd48103",
   "metadata": {},
   "source": [
    "### The Operating System - Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9c1a76-0863-4aaf-8ca9-f74fa39cd39a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'GetWeather' has no attribute 'model_json_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGetWeather\u001b[39;00m(BaseModel):\n\u001b[1;32m      7\u001b[0m     City: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m---> 10\u001b[0m get_weather \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_openai_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetWeather\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m weather_prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the weather in \u001b[39m\u001b[38;5;132;01m{place}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOpenAI()\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mmodel_to_openai_function\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_to_openai_function\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_json_schema\u001b[49m()}\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'GetWeather' has no attribute 'model_json_schema'"
     ]
    }
   ],
   "source": [
    "# The rain in spain\n",
    "# Tell the model the \"OS\" supports getting the weather\n",
    "\n",
    "\n",
    "# define a callable function\n",
    "class GetWeather(BaseModel):\n",
    "    City: str\n",
    "\n",
    "\n",
    "get_weather = model_to_openai_function(GetWeather)\n",
    "\n",
    "weather_prompt_template = \"What's the weather in {place}\"\n",
    "model = ChatOpenAI()\n",
    "weather_prompt = ChatPromptTemplate.from_template(weather_prompt_template)\n",
    "\n",
    "chain = weather_prompt | model.bind(\n",
    "    functions=[get_weather] # tell model we can call it.\n",
    ")  \n",
    "\n",
    "\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416bd24-8278-49df-ba06-84b2e592e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Woah  - Did you see  the bug?\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Do \"some more programming\"\n",
    "weather_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"When an API takes a city, infer an appropritiate city\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(weather_prompt_template),\n",
    "    ]\n",
    ")\n",
    "chain = weather_prompt | model.bind(functions=[get_weather])\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(\"Output\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489308bf-a7dc-43fb-adf8-473db4e9adf6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Back to our functions\n",
    "\n",
    "weather_with_data = weather_prompt.copy()\n",
    "\n",
    "# Update prompt with AI's desire to call a function\n",
    "weather_with_data.append(response)\n",
    "\n",
    "# Need to make tomorrow's cut, just stamp this please :)\n",
    "# Will come back and make a dispatcher and call actual functions\n",
    "\n",
    "weather_with_data.append(\n",
    "    FunctionMessage(name=\"GetWeather\", content=\"5 degrees and rainy\")\n",
    ")\n",
    "\n",
    "print(weather_with_data)\n",
    "\n",
    "\n",
    "chain = weather_with_data | model.bind(\n",
    "    functions=[get_weather]\n",
    ")  # tell model we can call it.\n",
    "\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print_line()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430962af-0803-4277-af45-aa1688cfcdfa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - Why do we seperate view from model?\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str\n",
    "    punch_line: str\n",
    "    reason_joke_is_funny: str\n",
    "\n",
    "\n",
    "class GetJokes(BaseModel):\n",
    "    count: int\n",
    "    jokes: List[Joke]\n",
    "\n",
    "\n",
    "get_jokes = model_to_openai_function(GetJokes)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "chain = prompt | model.bind(functions=[get_jokes]) | JsonOutputFunctionsParser2()\n",
    "print(prompt.messages)\n",
    "response = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "print(\"Output\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a7f7e-eee7-4abe-a325-386fb8d7bcf8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - What's better then doing math with a calculator?\n",
    "\n",
    "\n",
    "solve_math_with_python = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Write code to solve the users problem. the last line of the python  program should print the answer. Do not use sympy\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"What is the 217th prime\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ExecutePythonCode(BaseModel):\n",
    "    valid_python: str\n",
    "    code_explanation: str\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\").bind(\n",
    "    function_call={\"name\": \"ExecutePythonCode\"},  # tell gpt to use this model\n",
    "    functions=[model_to_openai_function(ExecutePythonCode)],\n",
    ")\n",
    "\n",
    "\n",
    "# JsonOutputFunctionParser2 == PrettyPrintOutput\n",
    "\n",
    "chain = solve_math_with_python | model | JsonOutputFunctionsParser2()\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response[\"code_explanation\"])\n",
    "print_line()\n",
    "\n",
    "valid_python = response[\"valid_python\"]\n",
    "print(valid_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6cb89-edf8-4285-93e1-321a94b7888c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ugh ... are you sure you want to do this??\n",
    "print_line()\n",
    "# input(\"Are you sure you want to run this code??\")\n",
    "exec(valid_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873feb1-d320-4e7e-992d-ff32737a33a5",
   "metadata": {},
   "source": [
    "### Operating System - Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093eac8-3709-4410-9f24-5a2b54f5a216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory - # no memory\n",
    "model = ChatOpenAI().bind(temperature=0)\n",
    "prompt_template = \"Tell me another joke\"\n",
    "last_prompt = \"\"\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    print (f\"Prompt {i}\\n\", prompt.messages)\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({})\n",
    "    \n",
    "    print(f\"Response {i}\\n\", response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa0d47-c408-4514-8463-b9fac7e6bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The memory module!\n",
    "\n",
    "from langchain.memory import  ChatMessageHistory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "# memory.save_context({\"input\":\"I like software engineer jokes\"} , {\"output\":\"That's odd, but fine\"})\n",
    "memory.add_user_message(\"I like software engineering jokes\") \n",
    "memory.add_ai_message(\"OK!\")\n",
    "print (memory.messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912f985-8b0c-4bec-b56b-557a672b3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory w/Memory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "memory.add_user_message(\"I like software engineering jokes\") \n",
    "memory.add_ai_message(\"OK!\")\n",
    "\n",
    "# human always says the same thing\n",
    "human_says = \"tell me another joke\"\n",
    "prompt = \"\" # have it outside loop so we can print it\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "\n",
    "    # build prompt, we'll print the last one ...    \n",
    "    prompt = ChatPromptTemplate.from_messages(memory.messages)\n",
    "    prompt.append(human_says) \n",
    "    # run the chain \n",
    "    chain = prompt | model\n",
    "    \n",
    "    response = chain.invoke({})\n",
    "    print(f\"Response {i}\\n\", response.content)\n",
    "\n",
    "    # store history\n",
    "    memory.add_user_message(human_says)\n",
    "    memory.add_ai_message(response.content)\n",
    "\n",
    "print_line()\n",
    "print_line()\n",
    "print(f\"Final Prompt\\n\", prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1258e-2922-4926-ae73-8415e19b153d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11df5ec-0ebb-47c8-b374-0befb237b82b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c249b6-49a8-44a5-841b-aa57eb4a94eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbe383-b22d-43c6-9301-8013b62acdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb05a8-3a97-44d0-9952-60ada8f82840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
