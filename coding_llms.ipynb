{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f14dcd-8e57-48d7-a3be-a0eec0383d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Coding with LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db2cdd-dbff-4a62-8824-ceb669184b9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ooodles of Imports\n",
    "import os\n",
    "import json\n",
    "from icecream import ic\n",
    "from rich.console import Console\n",
    "from rich import print\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "import pudb\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "console = Console()\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "from langchain.output_parsers.openai_functions import OutputFunctionsParser\n",
    "from langchain.schema import FunctionMessage\n",
    "\n",
    "\n",
    "from langchain.schema import (\n",
    "    Generation,\n",
    "    OutputParserException,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a8207-04ab-4ba5-8627-74a3539ec2e5",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful helpers\n",
    "def model_to_openai_function(cls):\n",
    "    return {\"name\": cls.__name__, \"parameters\": cls.model_json_schema()}\n",
    "\n",
    "\n",
    "class JsonOutputFunctionsParser2(OutputFunctionsParser):\n",
    "    \"\"\"Parse an output as the Json object.\"\"\"\n",
    "\n",
    "    def parse_result(self, result: List[Generation]) -> Any:\n",
    "        function_call_info = super().parse_result(result)\n",
    "        if self.args_only:\n",
    "            try:\n",
    "                # Waiting for this to merge upstream\n",
    "                return json.loads(function_call_info, strict=False)\n",
    "            except (json.JSONDecodeError, TypeError) as exc:\n",
    "                raise OutputParserException(\n",
    "                    f\"Could not parse function call data: {exc}\"\n",
    "                )\n",
    "        function_call_info[\"arguments\"] = json.loads(function_call_info[\"arguments\"])\n",
    "        return function_call_info\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def print_line():\n",
    "    display(HTML(\"<hr>\"))\n",
    "\n",
    "\n",
    "def print_prompt(prompt):\n",
    "    print(\"Prompt:\")\n",
    "    for m in prompt.messages:\n",
    "        print(f\"{type(m)}  {m.prompt}\")\n",
    "def print_function_call(response):\n",
    "    # #     additional_kwargs={'function_call': {'name': 'GetWeather', 'arguments': '{\\n\"City\": \"Spain\"\\n}'}},\n",
    "    function = response.additional_kwargs[\"function_call\"]\n",
    "    print_line()\n",
    "    print (f\"Call: {function['name']}\")\n",
    "    print (function['arguments'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3014a-0e96-4731-a244-58c50db48608",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Langchain - Super cool, we'll use it, but not our focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3ee05-94d2-44b3-a8d9-c794c48a0904",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prompts And Models - CPU and compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e44682-41ec-4eac-a7c8-30c7ba8b6857",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Everyone wants to be a comedian\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessage, \n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "print(joke_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff6eba-d81a-41e4-ab14-f6241f93aecc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the program and run on a familiar GPU\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = joke_prompt | model\n",
    "\n",
    "# Run it => Invoke()\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "\n",
    "# Show output\n",
    "print(result)\n",
    "print_line()\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45f74c-1c0a-47a8-929b-e2e2482349ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compile the program and run on our CPU\n",
    "\n",
    "local_model = GPT4All(model=\"./llama2.bin\")\n",
    "local_chain = joke_prompt | local_model\n",
    "\n",
    "# Run it => Invoke()\n",
    "topic = \"Software Engineers\"\n",
    "count = 2\n",
    "result = local_chain.invoke({\"topic\": topic, \"count\": count})\n",
    "\n",
    "# Show output\n",
    "print_line()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19dcc4-640e-4ffd-8167-263cafd48103",
   "metadata": {},
   "source": [
    "### Functions - The I/O System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c1a76-0863-4aaf-8ca9-f74fa39cd39a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The rain in spain\n",
    "# Tell the model the \"OS\" supports getting the weather\n",
    "\n",
    "\n",
    "# define a callable function\n",
    "class GetWeather(BaseModel):\n",
    "    City: str\n",
    "\n",
    "\n",
    "get_weather = model_to_openai_function(GetWeather)\n",
    "\n",
    "weather_prompt_template = \"What's the weather in {place}\"\n",
    "model = ChatOpenAI()\n",
    "weather_prompt = ChatPromptTemplate.from_template(weather_prompt_template)\n",
    "\n",
    "chain = weather_prompt | model.bind(\n",
    "    functions=[get_weather]  # tell model we can call it.\n",
    ")\n",
    "\n",
    "\n",
    "print (\"Show the full response:\")\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(response)\n",
    "\n",
    "print (\"Include an output parser in the chain:\")\n",
    "print_function_call(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416bd24-8278-49df-ba06-84b2e592e2fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Woah  - Did you see  the bug?\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Do \"some more programming\"\n",
    "weather_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"When an API takes a city, infer an appropritiate city\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(weather_prompt_template),\n",
    "    ]\n",
    ")\n",
    "chain = weather_prompt | model.bind(functions=[get_weather])\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print(\"Output\")\n",
    "print(response)\n",
    "print_line()\n",
    "print (response.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489308bf-a7dc-43fb-adf8-473db4e9adf6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Back to our functions\n",
    "\n",
    "weather_with_data = weather_prompt.copy()\n",
    "\n",
    "# Update prompt with AI's desire to call a function\n",
    "weather_with_data.append(response)\n",
    "\n",
    "# Need to make tomorrow's cut, just stamp this please :)\n",
    "# Will come back and make a dispatcher and call actual functions\n",
    "\n",
    "weather_with_data.append(\n",
    "    FunctionMessage(name=\"GetWeather\", content=\"5 degrees and rainy\")\n",
    ")\n",
    "\n",
    "print(weather_with_data)\n",
    "\n",
    "\n",
    "chain = weather_with_data | model.bind(\n",
    "    functions=[get_weather]\n",
    ")  # tell model we can call it.\n",
    "\n",
    "response = chain.invoke({\"place\": \"Spain\"})\n",
    "print_line()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430962af-0803-4277-af45-aa1688cfcdfa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - Why do we seperate view from model?\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str\n",
    "    punch_line: str\n",
    "    reason_joke_is_funny: str\n",
    "\n",
    "\n",
    "class GetJokes(BaseModel):\n",
    "    count: int\n",
    "    jokes: List[Joke]\n",
    "\n",
    "\n",
    "get_jokes = model_to_openai_function(GetJokes)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me {count} jokes about {topic}\")\n",
    "chain = prompt | model.bind(functions=[get_jokes]) | JsonOutputFunctionsParser2()\n",
    "print(prompt.messages)\n",
    "response = chain.invoke({\"topic\": topic, \"count\": count})\n",
    "print(\"Output\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a7f7e-eee7-4abe-a325-386fb8d7bcf8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - What's better then doing math with a calculator?\n",
    "\n",
    "\n",
    "solve_math_with_python = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Write code to solve the users problem. the last line of the python  program should print the answer. Do not use sympy\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"What is the 217th prime\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ExecutePythonCode(BaseModel):\n",
    "    valid_python: str\n",
    "    code_explanation: str\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\").bind(\n",
    "    function_call={\"name\": \"ExecutePythonCode\"},  # tell gpt to use this model\n",
    "    functions=[model_to_openai_function(ExecutePythonCode)],\n",
    ")\n",
    "\n",
    "\n",
    "# JsonOutputFunctionParser2 == PrettyPrintOutput\n",
    "\n",
    "chain = solve_math_with_python | model | JsonOutputFunctionsParser2()\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response[\"code_explanation\"])\n",
    "print_line()\n",
    "\n",
    "valid_python = response[\"valid_python\"]\n",
    "print(valid_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6cb89-edf8-4285-93e1-321a94b7888c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Woah Nelly  ... are you sure you want to do this??\n",
    "print_line()\n",
    "# input(\"Are you sure you want to run this code??\")\n",
    "exec(valid_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873feb1-d320-4e7e-992d-ff32737a33a5",
   "metadata": {},
   "source": [
    "### Memory - The memory System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093eac8-3709-4410-9f24-5a2b54f5a216",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory - # no memory\n",
    "model = ChatOpenAI().bind(temperature=0)\n",
    "prompt_template = \"Tell me another joke\"\n",
    "last_prompt = \"\"\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    print(f\"Prompt {i}\\n\", prompt.messages)\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({})\n",
    "\n",
    "    print(f\"Response {i}\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa0d47-c408-4514-8463-b9fac7e6bfe4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# The memory module!\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "\n",
    "# memory.save_context({\"input\":\"I like software engineer jokes\"} , {\"output\":\"That's odd, but fine\"})\n",
    "memory.add_user_message(\"I like software engineering jokes\")\n",
    "memory.add_ai_message(\"OK!\")\n",
    "print(memory.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912f985-8b0c-4bec-b56b-557a672b3b0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Basics of Conversational Memory w/Memory\n",
    "\n",
    "memory = ChatMessageHistory()\n",
    "memory.add_user_message(\"I like software engineering jokes\")\n",
    "memory.add_ai_message(\"OK!\")\n",
    "\n",
    "# human always says the same thing\n",
    "human_says = \"tell me another joke\"\n",
    "prompt = \"\"  # have it outside loop so we can print it\n",
    "for i in range(4):\n",
    "    print_line()\n",
    "\n",
    "    # build prompt, we'll print the last one ...\n",
    "    prompt = ChatPromptTemplate.from_messages(memory.messages)\n",
    "    prompt.append(human_says)\n",
    "    # run the chain\n",
    "    chain = prompt | model\n",
    "\n",
    "    response = chain.invoke({})\n",
    "    print(f\"Response {i}\\n\", response.content)\n",
    "\n",
    "    # store history\n",
    "    memory.add_user_message(human_says)\n",
    "    memory.add_ai_message(response.content)\n",
    "\n",
    "print_line()\n",
    "print_line()\n",
    "print(f\"Final Prompt\\n\", prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11df5ec-0ebb-47c8-b374-0befb237b82b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Innovate - A new kind of lossy compression\n",
    "\n",
    "semantic_compression_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a compressing chat model, summarize the entire conversation into a paragaph\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for m in memory.messages:\n",
    "    semantic_compression_prompt.append(m)\n",
    "\n",
    "chain = semantic_compression_prompt | model\n",
    "\n",
    "response = chain.invoke({})\n",
    "print(\"Semantic Compression of Conversation \\n\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca5fe0-7eb8-48b9-8ab9-d9bff6020e7f",
   "metadata": {},
   "source": [
    "### Retrievel - Virtual Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e39b3c-2971-4083-af42-df5874921a43",
   "metadata": {},
   "source": [
    "### Planning - Workflow and Verifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5136af2f-a9e3-4b11-8e50-9ccfd768bc1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toggle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m application:\u001b[43mtoggle\u001b[49m\u001b[38;5;241m-\u001b[39mmode\n",
      "\u001b[0;31mNameError\u001b[0m: name 'toggle' is not defined"
     ]
    }
   ],
   "source": [
    "application:toggle-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeaac57c-dd9f-4b2b-bfdc-580a3c3a4495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c6597-662a-4c39-bf1e-9301d432ff95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
