{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play TF/IFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import datasets, svm, metrics\n",
    "from pandas import DataFrame\n",
    "import matplotlib as mpl\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_paths = [\n",
    "    \"~/gits/igor2/750words/2019*md\",\n",
    "    \"~/gits/igor2/750words/2018*md\",\n",
    "    \"~/gits/igor2/750words_archive/*2012*txt\",\n",
    "    \"~/gits/igor2/750words_archive/*2013*txt\",\n",
    "    \"~/gits/igor2/750words_archive/*2014*txt\",\n",
    "    \"~/gits/igor2/750words_archive/*2015*txt\",\n",
    "    \"~/gits/igor2/750words_archive/*2016*txt\",\n",
    "    \"~/gits/igor2/750words_archive/*2017*txt\",\n",
    "    \"~/gits/igor2/750words_archive/*2018*txt\",\n",
    "    \"~/gits/igor2/750words/2019-01-*md\",\n",
    "    \"~/gits/igor2/750words/2019-02-*md\",\n",
    "    \"~/gits/igor2/750words/2019-03-*md\",\n",
    "    \"~/gits/igor2/750words/2019-04-*md\",\n",
    "    \"~/gits/igor2/750words/2019-05*md\",\n",
    "    \"~/gits/igor2/750words/2019-06-*md\",\n",
    "]\n",
    "\n",
    "\n",
    "def path_glob_to_string_of_words(path):\n",
    "    path_expanded = os.path.expanduser(path)\n",
    "    files = glob.glob(path_expanded)\n",
    "    # Make single string from all the file contents.\n",
    "    list_file_content = [Path(file).read_text() for file in files]\n",
    "    all_file_content = \" \".join(list_file_content)\n",
    "    return all_file_content\n",
    "\n",
    "\n",
    "corpus = [path_glob_to_string_of_words(p) for p in corpus_paths]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# I can skip stop words because I'm going to use TF/IDF\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.get_shape()[0] == len(corpus_paths), \"There should be a row per corpus path\"\n",
    "feature_labels = vectorizer.get_feature_names()\n",
    "# Should be a column per word. aka Huge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(use_idf=True, smooth_idf=False)\n",
    "Y = transformer.fit_transform(X)\n",
    "DataFrame(transformer.idf_, index=feature_labels)[0].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "# At this point, every column is the IF/TDF of the words in the document.\n",
    "# In theory the largest elements of the array would be the biggests.\n",
    "assert (\n",
    "    len(feature_labels) == Y[0].get_shape()[1]\n",
    "), \"Should have a laber for each column \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(Y.toarray().transpose(), index=feature_labels, columns=corpus_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(10)[-8:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20\n",
    "for x in df.columns:\n",
    "    print(df[x].sort_values(ascending=False)[start : start + 100])\n",
    "# df.iloc[:,1].sort_values(ascending=False)[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataFrame()\n",
    "?d.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
